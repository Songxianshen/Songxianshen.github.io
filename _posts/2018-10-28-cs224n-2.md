---
title: 【NLP】【CS224N】2 word2vec
date: 2018-10-28 19:42:31
---

![cs224n-2-1](/images/DL-images/cs224n-2-1.jpg)
![cs224n-2-2](/images/DL-images/cs224n-2-2.jpg)

独热词汇向量是一种存储在某处的本地表示。  

## what's word2vec?

一种通用的方法来学习神经词嵌入的问题。  
1. 定义一个根据中心词汇预测上下文的词汇（概率的方法，给定单词预测上下文单词出现的概率）
2. 用损失函数来判断预测的准确性
3. 调整词向量使得损失函数最小化

word2vec 模型的核心是构建一个很简单的、可扩展的、快速的训练模型，让我们可以处理数十亿单词的文本，并生成非常棒的单词表示。

#### Mian idea of word2vec
Predict between every word and its context words!  

Two algorithms  
1. Skip-grams(SG)
  Predict context words given target (position independent)
2. Continuous Bag of Word(CBOW)
  Predict target word from bag-of-words context

##### Skip-gram prediction
![skip-grams](/images/DL-images/cs224n-2-skip-gram.png)  
Skip-grams的概念是在每个估算步都取一个词作为中心词汇。  
这个模型将定义一个概率分布，即给定一个中心词汇，某个单词在他上下文出现的概率。  
我们会选取词汇的向量表示,以让概率分布最大化.  

这个模型只有一个概率分布,对于一个词汇,有且仅有一个概率分布.  

Details of word2vec:  
1. 对于每个单词t,预测周围单词. 范围为从中心词开始,到距离为m的位置.  
2. Objective function: Maximize the probability of any context word given the center word:  
![function](/images/DL-images/cs224n-2-skip-gram-1.png)  
theta是词汇向量的表示.

objective function = cost function = loss function  


![skip-grams model](/images/DL-images/cs224n-2-skip-gram-model.png)
